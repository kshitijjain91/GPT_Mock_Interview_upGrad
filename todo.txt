Phase-1: Launch MVP Friday April 7
- Create clear git workflow for collaborative development
- Deploy app on Huggingface spaces 
- Figure out a way to store audio files on server
- Store user data (email ID / pull from LMS)
- Set up cloud DB - store all user and bot conversations, scores and feedback; username also?
- Raise exception if openAI error [APIConnectionError]; raise gradio error (whisper / ChatCompletion / Completion)
- Basic CSS/visual changes: Increase font size; capitalize stuff in feedback
- Create DS and DM versions and MVP on LMS 
- Test internally: 8 people x 5 interviews per person (min 15 questions each) = 40 interviews
- Fine tune model on 40 interviews data
- Test fine tuned model again and launch on LMS 

Phase-2: Iterate and improve
- Consult someone from tech on this: Where should this be deployed, which DBs, should we launch on gradio and meanwhile build elsewhere etc 
- Speed up transcription (segmenting/compression/other techniques?)
- Streaming text input? Doesn't work too well. 
- Put text to speech? Only if speech is good quality like Murf.
- Fine tune: 
	- Provide common interview questions to GPT and guide GPT to ask questions in a representative flow; create rubric for each question so grading quality is better
	- Provide hints if unsure e.g. Hint: Think about things like normality, homoscedasticity, independence, and absence of multicollinearity. 
	- If response is imperfect, probe deeper on the same topic
	- Question - response pair is not exhaustive!
	- Test reliability: Should be 10/10 for perfect reponses; less than 4/10 for bad responses
- Improve UI on gradio
	- Add progress bar / some way to tell user how much interview is left
	- Hint functionality
	- P2 stuff:
		- Add flags 
		- Set difficulty level
		
________________________________________________________________________________________________________________________________________
## Macro (Long Term) Todo 
- Create plan for adoption and scale/impact (align with VG) 
- Build long term product plan: Ideally all data should be stored in a DB so we can fine tune the model continuously


